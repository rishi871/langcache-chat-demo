# ğŸ§  LangCache Chatbot Demo (FastAPI + Redis LangCache)

A minimal **FastAPI** chatbot demo that integrates **Redis LangCache** â€” a semantic cache for LLMs.  
It stores and retrieves answers to similar prompts, reducing repeated calls and making responses instant âš¡.

---

## âš™ï¸ What It Does

LangCache automatically caches the responses your LLM gives.  
When a new prompt arrives:
1. The app first **searches LangCache** for a similar question.  
2. If found â†’ returns the **cached answer instantly**.  
3. If not found â†’ calls the **LLM**, gets a new answer, and **stores** it back in LangCache.

This improves latency and reduces LLM costs.

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Install dependencies
```bash
pip install fastapi uvicorn openai langcache python-dotenv
